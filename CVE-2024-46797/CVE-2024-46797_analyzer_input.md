# Vulnerability Information: CVE-2024-46797

## Vulnerability Description
In the Linux kernel, the following vulnerability has been resolvedpowerpc/qspinlock Fix deadlock in MCS queueIf an interrupt occurs in queued_spin_lock_slowpath() after we incrementqnodesp->count and before node->lock is initialized, another CPU mightsee stale lock values in get_tail_qnode(). If the stale lock value happensto match the lock on that CPU, then we write to the next pointer ofthe wrong qnode. This causes a deadlock as the former CPU, once it becomesthe head of the MCS queue, will spin indefinitely until its next pointeris set by its successor in the queue.Running stress-ng on a 16 core (16EC/16VP) shared LPAR, results inoccasional lockups similar to the following $ stress-ng --all 128 --vm-bytes 80% --aggressive \ --maximize --oomable --verify --syslog \ --metrics --times --timeout 5m watchdog CPU 15 Hard LOCKUP ...... NIP [c0000000000b78f4] queued_spin_lock_slowpath+0x1184/0x1490 LR [c000000001037c5c] _raw_spin_lock+0x6c/0x90 Call Trace 0xc000002cfffa3bf0 (unreliable) _raw_spin_lock+0x6c/0x90 raw_spin_rq_lock_nested.part.135+0x4c/0xd0 sched_ttwu_pending+0x60/0x1f0 __flush_smp_call_function_queue+0x1dc/0x670 smp_ipi_demux_relaxed+0xa4/0x100 xive_muxed_ipi_action+0x20/0x40 __handle_irq_event_percpu+0x80/0x240 handle_irq_event_percpu+0x2c/0x80 handle_percpu_irq+0x84/0xd0 generic_handle_irq+0x54/0x80 __do_irq+0xac/0x210 __do_IRQ+0x74/0xd0 0x0 do_IRQ+0x8c/0x170 hardware_interrupt_common_virt+0x29c/0x2a0 --- interrupt 500 at queued_spin_lock_slowpath+0x4b8/0x1490 ...... NIP [c0000000000b6c28] queued_spin_lock_slowpath+0x4b8/0x1490 LR [c000000001037c5c] _raw_spin_lock+0x6c/0x90 --- interrupt 500 0xc0000029c1a41d00 (unreliable) _raw_spin_lock+0x6c/0x90 futex_wake+0x100/0x260 do_futex+0x21c/0x2a0 sys_futex+0x98/0x270 system_call_exception+0x14c/0x2f0 system_call_vectored_common+0x15c/0x2ecThe following code flow illustrates how the deadlock occurs.For the sake of brevity, assume that both locks (A and B) arecontended and we call the queued_spin_lock_slowpath() function. CPU0 CPU1 ---- ---- spin_lock_irqsave(A) | spin_unlock_irqrestore(A) | spin_lock(B) | | | ? | id = qnodesp->count++ | (Note that nodes[0].lock == A) | | | ? | Interrupt | (happens before nodes[0].lock = B) | | | ? | spin_lock_irqsave(A) | | | ? | id = qnodesp->count++ | nodes[1].lock = A | | | ? | Tail of MCS queue | | spin_lock_irqsave(A) ? | Head of MCS queue ? | CPU0 is previous tail ? | Spin indefinitely ? (until nodes[1].next != NULL) prev = get_tail_qnode(A, CPU0) | ? prev == &qnodes[CPU0].nodes[0] (as qnodes---truncated---

### Vulnerability Description Key Phrases
- **rootcause:** **A race condition in queued_spin_lock_slowpath() leads to a deadlock. Specifically, an interrupt occurring after incrementing qnodesp->count but before initializing node->lock can cause another CPU to read stale lock values from get_tail_qnode(). If this stale lock value matches the lock on the other CPU, it can result in incorrect next pointer assignment in the MCS queue, causing a deadlock.**
- **weakness:** **Race condition due to the lack of proper synchronization between incrementing the qnodesp->count and initializing node->lock in the queued_spin_lock_slowpath() function.**
- **impact:** Deadlock leading to system lockups and denial of service.
- **vector:** Concurrency issue triggered by interrupts in a multi-core environment.
- **attacker:** Not applicable, this is a bug.
- **product:** Linux Kernel
- **version:** Not specified in the provided text.
- **component:** powerpc/qspinlock

## CVE Reference Links Content Summary
The provided content relates to CVE-2024-46797.

**Root Cause:**
A race condition in the `queued_spin_lock_slowpath()` function of the PowerPC architecture's qspinlock implementation can lead to a deadlock. This occurs when an interrupt happens after the `qnodesp->count` is incremented but before the `node->lock` is initialized.

**Vulnerabilities/Weaknesses:**
- **Race Condition:** The core issue lies in the timing window between incrementing the queue counter and initializing the lock field in the MCS queue node. An interrupt during this window can lead to inconsistent state.
- **Stale Lock Values:** Another CPU might read stale lock values from the queue due to the race condition.
- **Incorrect Next Pointer:** When a stale lock value is read, the next pointer of the wrong qnode might get written to, creating a circular dependency.
- **MCS Queue Deadlock:** When the initial CPU becomes the head of the MCS queue, it spins indefinitely because its next pointer was never correctly set.

**Impact of Exploitation:**
The vulnerability results in a system deadlock. The affected CPU becomes unresponsive, leading to a hard lockup. This can impact the overall system stability and availability, potentially requiring a system reboot to recover.

**Attack Vectors:**
- **Interrupt Timing:** The exploit relies on a carefully timed interrupt that occurs during the critical window in `queued_spin_lock_slowpath()`.
- **Contended Locks:** The deadlock is more likely to happen under high contention for multiple locks as this triggers the slow path of spin lock which utilizes the mentioned queue.

**Required Attacker Capabilities/Position:**
- The attacker needs to trigger a specific sequence of operations and timing to cause the interrupt at the right moment to trigger the race condition.
- The attacker would need to be able to execute code on the affected system.

**Technical Details:**
The fix involves clearing the `node->lock` before releasing the node and inserting a memory barrier to prevent further stores to the node after it has been released. This ensures that no CPU sees stale values if an interrupt occurs after `qnodesp->count` is incremented but before `node->lock` is initialized. The critical code change is:

```c
release:
    /*
     * Clear the lock before releasing the node, as another CPU might see stale
     * values if an interrupt occurs after we increment qnodesp->count
     * but before node->lock is initialized. The barrier ensures that
     * there are no further stores to the node after it has been released.
     */
    node->lock = NULL;
    barrier();
    qnodesp->count--;
```
The provided content offers more details than a typical CVE description.

## Retriever Results

### Top Combined Results

| Rank | CWE ID | Name | Abstraction | Usage  | Retrievers | Individual Scores |
|------|--------|------|-------------|-------|------------|-------------------|
| 1 | 362 | Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition') | Class | Allowed-with-Review | alternate_terms | 0.800 |
| 2 | 667 | Improper Locking | Class | Allowed-with-Review | sparse | 1.074 |
| 3 | 833 | Deadlock | Base | Allowed | sparse | 1.073 |
| 4 | 451 | User Interface (UI) Misrepresentation of Critical Information | Class | Allowed-with-Review | sparse | 0.968 |
| 5 | 835 | Loop with Unreachable Exit Condition ('Infinite Loop') | Base | Allowed | sparse | 0.958 |
| 6 | 1390 | Weak Authentication | Class | Allowed-with-Review | sparse | 0.949 |
| 7 | 319 | Cleartext Transmission of Sensitive Information | Base | Allowed | sparse | 0.938 |
| 8 | 400 | Uncontrolled Resource Consumption | Class | Discouraged | sparse | 0.927 |
| 9 | 413 | Improper Resource Locking | Base | Allowed | dense | 0.556 |
| 10 | 476 | NULL Pointer Dereference | Base | Allowed | graph | 0.002 |

