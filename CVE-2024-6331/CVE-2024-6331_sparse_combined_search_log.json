{
  "method": "sparse_combined",
  "query": "stitionai/devika main branch as of commit cdfb782b0e634b773b10963c8034dc9207ba1f9f is vulnerable to Local File Read (LFI) by Prompt Injection. The integration of Google Gimini 1.0 Pro with `HarmBlockThreshold.BLOCK_NONE` for `HarmCategory.HARM_CATEGORY_HATE_SPEECH` and `HarmCategory.HARM_CATEGORY_HARASSMENT` in `safety_settings` disables content protection. This allows malicious commands to be executed, such as reading sensitive file contents like `/etc/passwd`.",
  "keyphrases": {
    "rootcause": [
      "Prompt Injection"
    ]
  },
  "timestamp": "2025-07-12T07:14:51.139676",
  "results_count": 13,
  "results_summary": [
    {
      "doc_id": "1427",
      "name": "Improper Neutralization of Input Used for LLM Prompting",
      "score": 1000.0
    },
    {
      "doc_id": "116",
      "name": "Improper Encoding or Escaping of Output",
      "score": 262.1572017837193
    },
    {
      "doc_id": "611",
      "name": "Improper Restriction of XML External Entity Reference",
      "score": 261.8610654792248
    },
    {
      "doc_id": "918",
      "name": "Server-Side Request Forgery (SSRF)",
      "score": 258.74116532649055
    },
    {
      "doc_id": "24",
      "name": "Path Traversal: '../filedir'",
      "score": 248.3744461806312
    }
  ]
}