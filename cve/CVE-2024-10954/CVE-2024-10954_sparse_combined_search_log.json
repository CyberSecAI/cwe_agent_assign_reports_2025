{
  "method": "sparse_combined",
  "query": "In the `manim` plugin of binary-husky/gpt_academic, versions prior to the fix, a vulnerability exists due to improper handling of user-provided prompts. The root cause is the execution of untrusted code generated by the LLM without a proper sandbox. This allows an attacker to perform remote code execution (RCE) on the app backend server by injecting malicious code through the prompt.",
  "keyphrases": {
    "rootcause": [
      "improper handling of user-provided prompts"
    ]
  },
  "timestamp": "2025-07-13T00:39:38.448815",
  "results_count": 12,
  "results_summary": [
    {
      "doc_id": "1427",
      "name": "Improper Neutralization of Input Used for LLM Prompting",
      "score": 411.42344502689093
    },
    {
      "doc_id": "94",
      "name": "Improper Control of Generation of Code ('Code Injection')",
      "score": 377.7175927545437
    },
    {
      "doc_id": "1284",
      "name": "Improper Validation of Specified Quantity in Input",
      "score": 368.9058646811337
    },
    {
      "doc_id": "20",
      "name": "Improper Input Validation",
      "score": 368.20412121668863
    },
    {
      "doc_id": "138",
      "name": "Improper Neutralization of Special Elements",
      "score": 367.54887231647814
    }
  ]
}