{
  "method": "sparse_base",
  "query": "In the Linux kernel, the following vulnerability has been resolved fix bitmap corruption on close_range() with CLOSE_RANGE_UNSHARE copy_fd_bitmaps(new, old, count) is expected to copy the first count/BITS_PER_LONG bits from old->full_fds_bits[] and fill the rest with zeroes. What it does is copying enough words (BITS_TO_LONGS(count/BITS_PER_LONG)), then memsets the rest. That works fine, *if* all bits past the cutoff point are clear. Otherwise we are risking garbage from the last word wed copied. For most of the callers that is true - expand_fdtable() has count equal to old->max_fds, so theres no open descriptors past count, let alone fully occupied words in ->open_fds[], which is what bits in ->full_fds_bits[] correspond to. The other caller (dup_fd()) passes sane_fdtable_size(old_fdt, max_fds), which is the smallest multiple of BITS_PER_LONG that covers all opened descriptors below max_fds. In the common case (copying on fork()) max_fds is ~0U, so all opened descriptors will be below it and we are fine, by the same reasons why the call in expand_fdtable() is safe. Unfortunately, there is a case where max_fds is less than that and where we might, indeed, end up with junk in ->full_fds_bits[] - close_range(from, to, CLOSE_RANGE_UNSHARE) with * descriptor table being currently shared * to being above the current capacity of descriptor table * from being just under some chunk of opened descriptors. In that case we end up with observably wrong behaviour - e.g. spa",
  "keyphrases": {
    "base_query": "In the Linux kernel, the following vulnerability has been resolved fix bitmap corruption on close_range() with CLOSE_RANGE_UNSHARE copy_fd_bitmaps(new, old, count) is expected to copy the first count/BITS_PER_LONG bits from old->full_fds_bits[] and fill the rest with zeroes. What it does is copying enough words (BITS_TO_LONGS(count/BITS_PER_LONG)), then memsets the rest. That works fine, *if* all bits past the cutoff point are clear. Otherwise we are risking garbage from the last word wed copied. For most of the callers that is true - expand_fdtable() has count equal to old->max_fds, so theres no open descriptors past count, let alone fully occupied words in ->open_fds[], which is what bits in ->full_fds_bits[] correspond to. The other caller (dup_fd()) passes sane_fdtable_size(old_fdt, max_fds), which is the smallest multiple of BITS_PER_LONG that covers all opened descriptors below max_fds. In the common case (copying on fork()) max_fds is ~0U, so all opened descriptors will be below it and we are fine, by the same reasons why the call in expand_fdtable() is safe. Unfortunately, there is a case where max_fds is less than that and where we might, indeed, end up with junk in ->full_fds_bits[] - close_range(from, to, CLOSE_RANGE_UNSHARE) with * descriptor table being currently shared * to being above the current capacity of descriptor table * from being just under some chunk of opened descriptors. In that case we end up with observably wrong behaviour - e.g. spa"
  },
  "timestamp": "2025-07-13T15:54:54.100462",
  "results_count": 10,
  "results_summary": [
    {
      "doc_id": "775",
      "name": "Missing Release of File Descriptor or Handle after Effective Lifetime",
      "score": 334.46450247507875
    },
    {
      "doc_id": "911",
      "name": "Improper Update of Reference Count",
      "score": 327.2154025260051
    },
    {
      "doc_id": "190",
      "name": "Integer Overflow or Wraparound",
      "score": 321.0121007016332
    },
    {
      "doc_id": "362",
      "name": "Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')",
      "score": 319.96447311988055
    },
    {
      "doc_id": "1284",
      "name": "Improper Validation of Specified Quantity in Input",
      "score": 317.28617111226527
    }
  ]
}