{
  "method": "sparse_altterm_rootcause_prompt inj",
  "query": "A prompt injection vulnerability in the chatbox of Netangular Technologies ChatNet AI Version v1.0 allows attackers to access and exfiltrate all previous and subsequent chat data between the user and the AI assistant via a crafted message.",
  "keyphrases": {
    "rootcause": "prompt injection"
  },
  "timestamp": "2025-07-13T18:44:33.355993",
  "results_count": 1,
  "results_summary": [
    {
      "doc_id": "1427",
      "name": "Improper Neutralization of Input Used for LLM Prompting",
      "score": 1000.0
    }
  ]
}