{
  "method": "sparse_base",
  "query": "picklescan before 0.0.21 does not treat pip as an unsafe global. An attacker could craft a malicious model that uses Pickle to pull in a malicious PyPI package (hosted, for example, on pypi.org or GitHub) via `pip.main()`. Because pip is not a restricted global, the model, when scanned with picklescan, would pass security checks and appear to be safe, when it could instead prove to be problematic.",
  "keyphrases": {
    "base_query": "picklescan before 0.0.21 does not treat pip as an unsafe global. An attacker could craft a malicious model that uses Pickle to pull in a malicious PyPI package (hosted, for example, on pypi.org or GitHub) via `pip.main()`. Because pip is not a restricted global, the model, when scanned with picklescan, would pass security checks and appear to be safe, when it could instead prove to be problematic."
  },
  "timestamp": "2025-07-12T09:14:00.075178",
  "results_count": 10,
  "results_summary": [
    {
      "doc_id": "497",
      "name": "Exposure of Sensitive System Information to an Unauthorized Control Sphere",
      "score": 126.71456152741808
    },
    {
      "doc_id": "1333",
      "name": "Inefficient Regular Expression Complexity",
      "score": 124.2606124188241
    },
    {
      "doc_id": "23",
      "name": "Relative Path Traversal",
      "score": 118.8085054652656
    },
    {
      "doc_id": "138",
      "name": "Improper Neutralization of Special Elements",
      "score": 118.67444921501706
    },
    {
      "doc_id": "88",
      "name": "Improper Neutralization of Argument Delimiters in a Command ('Argument Injection')",
      "score": 114.01545785304113
    }
  ]
}